# 01 - LLM Foundations

This module contains my foundational exploration of Large Language Models (LLMs).

## Goals

- Understand how LLMs work conceptually
- Explore transformers, attention and embeddings
- Connect theory to real-world backend applications
- Identify implications for AI system reliability and quality

---

## Topics Covered

- What is AI vs ML vs Deep Learning
- What is a Transformer architecture
- Self-attention mechanism
- Tokens and embeddings
- Inference vs Training

---

## Practical Experiments (Planned)

- Basic LLM API interaction
- Prompt structure experiments
- Response variability testing
- Token usage observation

---

## Notes

The objective is not just to understand theory, but to analyze how these components impact:

- Reliability
- Hallucination risk
- Determinism
- Observability in production systems
